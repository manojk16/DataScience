{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning by Andrew NG - Week_2_Quiz_1_Multi-variant_Linear_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Suppose  _m_=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| midterm exam  | (midterm exam)<sup>2</sup>  |  final exam |\n",
    "|---|---|---|\n",
    "| 89 |7921   |  96 |\n",
    "| 72  |5184   | 74  |\n",
    "| 94  |8836   | 87  |\n",
    "| 69 |4761   |  78 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'd like to use polynomial regression to predict a student's final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form $h_\\theta(x)$= $\\theta_0$ + $\\theta_1x_1$ + $\\theta_2 x_2$​, where $x_1$​ is the midterm score and $x_2$​ is (midterm score)<sup>2</sup>. Further, you plan to use both feature scaling (dividing by the \"max-min\", or range, of a feature) and mean normalization.\n",
    "What is the normalized feature $x_2^{(2)}$​? (Hint: midterm = 72, final = 74 is training example 2.) Please round off your answer to two decimal places and enter in the text box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ans: \n",
    "$x_2^{(2)}$​=5184<br/>\n",
    "mean of $x_2$=(7921+5184+8836+4761)/4=6675.5<br/>\n",
    "range        = (8836-4761)=4075<br/>\n",
    "normalized feature $x_2^{(2)}$=(5184-6675.5)/4075= -0.37<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 \n",
    "You run gradient descent for 15 iterations with  $\\alpha$ = $0.3$  and compute $J(\\theta)$ after each iteration. You find that the value of  $J(\\theta)$  **decreases slowly**  and is still decreasing after 15 iterations. Based on this, which of the following conclusions seems most plausible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[x] Rather than use the current value of $\\alpha$, it'd be more promising to try a larger value of $\\alpha$ (say $\\alpha$ = $1.0$).<br/>\n",
    "[ ] Rather than use the current value of $\\alpha$, it'd be more promising to try a smaller value of $\\alpha$ (say $\\alpha = 0.1$)<br/>\n",
    "[ ] α=0.3 is an effective choice of learning rate.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "uppose you have m = 23 training examples with n = 5 features (excluding the additional all-ones feature for the intercept term, which you should add). The normal equation is $\\theta$ = ($X^TX)^\n",
    "{-1}$$X^T$$y$. For the given values of m and n, what are the dimensions of $\\theta$, $X$ and $y$ in this equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [ ] $X$ is 23$\\times6$, $y$ is 23$\\times6$, $\\theta$ is $6\\times6$<br/>\n",
    " [x] $X$ is 23$\\times6$, $y$ is 23$\\times6$, $\\theta$ is $6\\times1$<br/>\n",
    " [ ] $X$ is 23$\\times5$, $y$ is 23$\\times6$, $\\theta$ is $5\\times1$<br/>\n",
    " [ ] $X$ is 23$\\times5$, $y$ is 23$\\times6$, $\\theta$ is $5\\times5$<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "Suppose you have a dataset with m = 1000000 examples and n = 200000 features for each example. You want to use multivariate linear regression to fit the parameters $\\theta$ to our data. Should you prefer gradient descent or the normal equation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ ] The normal equation, since it provides an efficient way to directly find the solution.<br/>\n",
    "[ ] Gradient descent, since it will always converge to the optimal  $\\theta$.<br/>\n",
    "[ ] The normal equation, since gradient descent might be unable to find the optimal  $\\theta$.<br/>\n",
    "[x] Gradient descent, since  $\\theta$ = ($X^TX)^\n",
    "{-1}$$X^T$$y$  will be very slow to compute in the normal equation.<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "   Which of the following are reasons for using feature scaling? \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [ ] It is necessary to prevent the normal equation from getting stuck in local optima.<br/>\n",
    " [x]  It speeds up gradient descent by making it require fewer iterations to get to a good solution.<br/>\n",
    " [ ]  It prevents the matrix  ($X^TX)$  (used in the normal equation) from being non-invertable (singular/degenerate).<br/>\n",
    " [ ]  It speeds up gradient descent by making each iteration of gradient descent less expensive to compute.<br/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
